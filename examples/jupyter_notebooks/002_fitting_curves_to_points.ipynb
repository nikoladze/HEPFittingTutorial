{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting curves to point clouds using numpy and scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this tutorial, we make our lives a bit easier and use some pre-defined fitting functions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:40:45.013004Z",
     "start_time": "2020-04-22T12:40:45.007487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Numerical operations:\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Minimizing/fitting library:\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Remember:</b> You can always get help about any function, e.g. \n",
    "    <code>np.array</code> \n",
    "by typing\n",
    "\n",
    "``np.array?``\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting lines or polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a polynomial to a point cloud.\n",
    "For this we use the ``polyfit`` function of numpy (https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyfit.html). \n",
    "\n",
    "Source: https://stackoverflow.com/questions/19165259/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining some points and their x and y projections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:42:34.274426Z",
     "start_time": "2020-04-22T12:42:34.259655Z"
    }
   },
   "outputs": [],
   "source": [
    "points = np.array([(1, 1), (2, 4), (3, 1), (9, 3), (10, 10)])\n",
    "x = points[:,0]\n",
    "y = points[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:43:56.267745Z",
     "start_time": "2020-04-22T12:43:56.258370Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise 1 [super-easy]:</b> Plot the points\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a polynomial of degree ``deg`` is easy as calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:44:19.189856Z",
     "start_time": "2020-04-22T12:44:19.178220Z"
    }
   },
   "outputs": [],
   "source": [
    "coeffs = np.polyfit(x, y, deg=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector ``coeffs`` now contains the coefficients of the polynomial as a vector of length ``deg+1``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:44:20.385394Z",
     "start_time": "2020-04-22T12:44:20.371128Z"
    }
   },
   "outputs": [],
   "source": [
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to also get the corresponding function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:44:21.493435Z",
     "start_time": "2020-04-22T12:44:21.487325Z"
    }
   },
   "outputs": [],
   "source": [
    "f = np.poly1d(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:45:02.631396Z",
     "start_time": "2020-04-22T12:45:02.623681Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise 2 [super-easy]:</b> Evaluate the difference $f(x)-y$ and interpret them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally have a look at the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:42:37.354471Z",
     "start_time": "2020-04-22T12:42:37.181766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some points where we evaluate our new function\n",
    "x_new = np.linspace(x[0], x[-1], 50)\n",
    "y_new = f(x_new)\n",
    "\n",
    "# Plot the datapoints\n",
    "plt.plot(x, y, 'ko', label=\"data\")\n",
    "\n",
    "# Plot our fitted polynomial\n",
    "plt.plot(x_new, y_new, 'r-', label=\"fit\")\n",
    "\n",
    "# Add legend etc\n",
    "plt.legend()\n",
    "plt.xlim([x[0]-1, x[-1] + 1 ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise 3 [easy]:</b> Fit a straight line to the following dataset and determine the line parameters. Bonus points: Also plot the fit as above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:47:38.053751Z",
     "start_time": "2020-04-22T12:47:38.044705Z"
    }
   },
   "outputs": [],
   "source": [
    "x_exercise3 = np.linspace(0, 1, 20)\n",
    "y_exercise3 = x_exercise3 + np.random.random(len(x_exercise3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_exercise3, y_exercise3, \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Gaussian (or any arbitrary function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://stackoverflow.com/questions/19206332"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again let's generate some points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:55:30.124413Z",
     "start_time": "2020-04-22T12:55:30.112267Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.asarray(range(10))\n",
    "y = 0.3*x + np.asarray([0,1,2,3,4,5,4,3,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define our own function that we want to fit to our data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T12:55:30.965057Z",
     "start_time": "2020-04-22T12:55:30.955193Z"
    }
   },
   "outputs": [],
   "source": [
    "def gaus(x, norm, mean, sigma):\n",
    "    # Note that this function takes a whole vector x of data!\n",
    "    return norm * np.exp(-(x-mean)**2/(2*sigma**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function has 3 free parameters, ``norm``, ``mean``, ``sigma``, which we will now fit using ``scipy.optimize.curve_fit`` (https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T13:00:16.259357Z",
     "start_time": "2020-04-22T13:00:16.249131Z"
    }
   },
   "outputs": [],
   "source": [
    "popt, pcov = scipy.optimize.curve_fit(gaus, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable ``popt`` holds the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T13:00:17.110038Z",
     "start_time": "2020-04-22T13:00:17.101412Z"
    }
   },
   "outputs": [],
   "source": [
    "popt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other variable, ``pcov`` holds the covariance matrix of the fitted values. \n",
    "This gets relevant once you want to give errors on your fitted quantities, but we'll ignore this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-22T13:00:52.501266Z",
     "start_time": "2020-04-22T13:00:52.332343Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot points\n",
    "plt.plot(x, y,'ko',label='data')\n",
    "\n",
    "# Plot our gaussian\n",
    "# Define some points on the xaxis\n",
    "x_fine = np.linspace(min(x), max(x), 200)\n",
    "y_fine = gaus(x_fine, *popt)\n",
    "# The *  unpacks the values from popt and uses them as\n",
    "# parameters.\n",
    "\n",
    "plt.plot(x_fine, y_fine, 'r-', label='fit')\n",
    "\n",
    "# ...\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "   <b>Exercise 4a [easy]:</b> \n",
    "Modify the above example so that the sigma above is \n",
    "    <b>fixed</b> to 1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "   <b>Exercise 4b [medium]:</b> \n",
    "Let's try to fit the same dataset with a slightly more complicated model that consists of a \n",
    "linear part plus a Gaussian, i.e. \n",
    "    \n",
    "```f(x) = x + b + gaus(x, norm, mean, sigma)```\n",
    "\n",
    "<b>Hint</b>:\n",
    "Create a new function ``mymodel`` which takes the parameters ``x``, as well as all fitted parameters, i.e. ``b``, ``norm``, ``mean``, ``sigma``. In the definition you can also use the function ``gaus`` from above!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "   <b>Exercise 4c [hard]:</b> \n",
    "Fit the above points as two lines, i.e. \n",
    "$$f(x) = \\begin{cases}\n",
    "  a_1 x + b_1 & \\text{if}\\ x<c\\\\\n",
    "  a_2 x + b_2 & \\text{if}\\ x\\geq c\\\\\n",
    "\\end{cases}$$\n",
    "    Such that $a_1 c + b_1 = a_2 c + b_2$.\n",
    "<p>\n",
    "<b>Hint:</b> \n",
    "<ul>\n",
    "<li>Solve the contraint for, say, <code>b2</code></li>\n",
    "<li>Define a function <code>line(x, a1, b1, a2, c)</code></li>\n",
    "<li>Note that <code>x</code> is a vector! Thus <code>if x&lt;c: ...</code> won't work. Rather, take a look at the function <code>np.where</code>.\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit uncertainties and covariances\n",
    "\n",
    "(taken from a [python ML course at LMU](https://github.com/fuenfundachtzig/LMU_DA_ML_Basic/blob/main/notebooks/Fitting.ipynb))\n",
    "\n",
    "Also taking some inspiration from http://www.pp.rhul.ac.uk/~cowan/stat_course.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix shows the variances on the fit parameters on the diagonal and their covariances on the off diagonal elements.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Since we did not provide uncertainties on the data points (would be done using the <code>sigma</code> argument to <code>curve_fit</code>) the overall normalization of $\\chi^2$ is undetermined. By default, `curve_fit` multiplies the covariance matrix by $\\chi^2_\\mathrm{min}/\\mathrm{ndf}$ which effectively scales the uncertainties on the data points to match the observed residuals after the fit. This can be turned off by using <code>absolute_sigma=True</code> which can be used when uncertainties are provided.\n",
    "</div>\n",
    "\n",
    "Let's go back to the example with the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "x_data = np.linspace(-1, 1, 10)\n",
    "y_data = -1 + 3 * x_data + 2 * np.random.random_sample(len(x_data))\n",
    "\n",
    "def f(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "pfit, pcov = curve_fit(f, x_data, y_data, p0=(1, 0))\n",
    "pfit, pcov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the one standard deviation errors on the fit parameters will be given by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = pfit\n",
    "a_err, b_err = np.sqrt(np.diag(pcov))\n",
    "print(\"a = {:.2f} +/- {:.2f}\".format(a, a_err))\n",
    "print(\"b = {:.2f} +/- {:.2f}\".format(b, b_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariances indicate how much the parameters are correlated, that is how much greater values of one parameter correspond to greater values of the other one.\n",
    "\n",
    "In this case the covariance is very small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcov[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try out why that is. Change on of the parameters slightly and see if there is a way to get a better `chi2` again by changing the other one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact(\n",
    "    a_test=(a - 0.5, a + 0.5, 0.05),\n",
    "    b_test=(b - 0.5, b + 0.5, 0.05),\n",
    "    continuous_update=False\n",
    ")\n",
    "def interactive_plot(a_test=a, b_test=b):\n",
    "    plt.plot(x_data, y_data, \"ko\")\n",
    "    yfit = f(x_data, a_test, b_test)\n",
    "    yfit_min = f(x_data, a, b)\n",
    "    plt.plot(x_data, yfit, label=\"test\")\n",
    "    plt.plot(x_data, yfit_min, \"--\", label=\"fit\")\n",
    "    plt.ylim(-4, 4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"Chi2:\", ((yfit - y_data) ** 2).sum())\n",
    "    print(\"Chi2_min:\", ((yfit_min - y_data) ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, if we fix a to a value slightly away from the optimum, and fit `b` again, the optimal `b` won't change significantly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_fit(lambda x, b: f(x, a + 1, b), x_data, y_data, p0=(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have the same number of points on the positive and negative x-axis, so a higher slope (parameter `a`) puts the data points on the positive x-axis below the fitted line and on the negative x-axis above the fitted line.\n",
    "\n",
    "Changing the parameter `b` will cause an overall shift to the top or bottom, so there is no way to compensate a less optimal value of `a`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The situation changes if we only look at data points on the positive x-axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data2 = np.linspace(0, 1, 10)\n",
    "y_data2 = -1 + 3 * x_data2 + np.random.random_sample(len(x_data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_data2, y_data2, \"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfit2, pcov2 = curve_fit(f, x_data2, y_data2, p0=(1, 0))\n",
    "pfit2, pcov2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Question:</b> What is the correlation coefficient?<br><br>\n",
    "    <b>Hint:</b> Have a look at <a href=\"https://github.com/gdxd/Stat_Root/blob/main/Stat/StatBasics.ipynb\">Günter's statistic introduction</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the off diagonal elements have a significant (negative) non-zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2, b2 = pfit2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compensate a slightly higher value of `a` by a slightly lower value of `b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact(\n",
    "    a_test=(a2 - 0.5, a2 + 0.5, 0.05),\n",
    "    b_test=(b2 - 0.5, b2 + 0.5, 0.05),\n",
    "    continuous_update=False\n",
    ")\n",
    "def interactive_plot(a_test=a2, b_test=b2):\n",
    "    plt.plot(x_data2, y_data2, \"ko\")\n",
    "    yfit = f(x_data2, a_test, b_test)\n",
    "    yfit_min = f(x_data2, a2, b2)\n",
    "    plt.plot(x_data2, yfit, label=\"test\")\n",
    "    plt.plot(x_data2, yfit_min, \"--\", label=\"fit\")\n",
    "    plt.ylim(-2, 4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"Chi2:\", ((yfit - y_data2) ** 2).sum())\n",
    "    print(\"Chi2_min:\", ((yfit_min - y_data2) ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_fit(lambda x, b: f(x, a2 + 1, b), x_data2, y_data2, p0=(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix gives an idea of how much the fitted values would spread if we were to repeat the fit with new random data, assuming our model describes it.\n",
    "\n",
    "Since we know the \"real\" distribution of our data points (we generated them after all) we can try this out with toy experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_toy():\n",
    "    x_data = np.linspace(0, 1, 10)\n",
    "    y_data = -1 + 3 * x_data + np.random.random_sample(len(x_data))\n",
    "    pfit, pcov = curve_fit(f, x_data, y_data, p0=(1, 0))\n",
    "    return pfit, pcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_params = np.array([fit_toy()[0] for i in range(5000)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(*toy_params, marker=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the empirical covariance matrix of these points to the one determined from the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cov(toy_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcov2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can do with this covariance matrix is to visualize uncertainties on the fit using [linear error propagation](https://en.wikipedia.org/wiki/Propagation_of_uncertainty). In this case we can simply calculate it manually:\n",
    "\n",
    "$$\\sigma_y ^ 2 = \\left(\\frac{\\partial y}{\\partial a} \\sigma_a \\right)^2 + \\left(\\frac{\\partial y}{\\partial b} \\sigma_b \\right)^2 + 2\\frac{\\partial y}{\\partial a}\\frac{\\partial y}{\\partial b}\\sigma_{ab}$$\n",
    "\n",
    "where $\\sigma_a, \\sigma_b$ are the variances and $\\sigma_{ab}$ is the covariance. With $y = ax + b$ this becomes:\n",
    "\n",
    "$$\\sigma_y ^ 2 = (x\\sigma_a)^2 + \\sigma_b^2 + 2x\\sigma_{ab}$$\n",
    "\n",
    "where we can take $\\sigma_a, \\sigma_b, \\sigma_{ab}$ from the covariance matrix:\n",
    "\n",
    "$$\n",
    "\\pmatrix{\n",
    "    \\sigma_a^2 & \\sigma_{ab} \\\\\n",
    "    \\sigma_{ba} & \\sigma_b^2\n",
    "}\n",
    "$$\n",
    "\n",
    "Let's visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_y = np.sqrt((x_data2 * np.sqrt(pcov2[0, 0])) ** 2 + pcov2[1, 1] + 2 * x_data2 * pcov2[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f(x_data2, *pfit2)\n",
    "plt.fill_between(x_data2, y - sigma_y, y + sigma_y, alpha=0.5)\n",
    "plt.plot(x_data2, y)\n",
    "plt.plot(x_data2, y_data2, \"ko\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more generic templates/functions you can do that automatically. Either use packages like [uncertainties](https://pythonhosted.org/uncertainties) (for functions described by simple formulas), [jacobi](https://github.com/hdembinski/jacobi) (for generic functions) or calculate it numerically by varying each parameter up and down and using half of the resulting interval as a replacement for $\\frac{\\partial f}{\\partial x_i}\\sigma_{x_i}$.\n",
    "\n",
    "Some tools like `RooFit` (see tutorial later) have functionality for this built in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at these data points (this time with uncertainties):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0])\n",
    "y_data = np.array([2.7, 3.9, 5.5, 5.8, 6.5, 6.3, 6.7, 6.2, 6.0])\n",
    "yerr_data = np.array([0.3, 0.5, 0.7, 0.6, 0.4, 0.3, 0.7, 0.8, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x_data, y_data, yerr=yerr_data, fmt=\"ko\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fit a line again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_linear(x, a, b):\n",
    "    return a * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfit, pcov = curve_fit(f_linear, x_data, y_data, sigma=yerr_data, absolute_sigma=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x_data, y_data, yerr=yerr_data, fmt=\"ko\")\n",
    "plt.plot(x_data, f_linear(x_data, *pfit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look very great. How can we quantify the quality of this fit? We look at our $\\chi^2$ statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_chi2(f, params, x, y, yerr):\n",
    "    return (((f(x, *params) - y) / yerr) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_chi2(f_linear, pfit, x_data, y_data, yerr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: As a rule of thumb, the number of degrees of freedom \"ndf\" (number of data points - number of parameters) should be roughly equal to the $\\chi^2$ statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_data) - len(pfit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this rule of thumb already indicates this is not a very nice fit. We can be even more quantitative. This rule comes from the fact that the $\\chi^2$ statistic actually follows a [Chi-squared distribution](https://en.wikipedia.org/wiki/Chi-squared_distribution) (which has ndf as expectation value) if we assume the data points follow a normal distribution. Using this we can calculate a p-value that answers the question \"how often would we get such a high value of $\\chi^2$ in repeated experiments, given that our function describes the data\".\n",
    "\n",
    "Scipy provides functions for common probability density functions among which there is also the chi-squared distribution. What we want to calculate is\n",
    "\n",
    "$$p = \\int\\limits_{\\chi^2_\\mathrm{min}}^{\\infty}f(\\chi^2, \\mathrm{ndf})\\mathrm{d}\\chi^2 = 1 - F(\\chi^2_\\mathrm{min}, \\mathrm{ndf})$$\n",
    "\n",
    "where $F(\\chi^2_\\mathrm{min}, \\mathrm{ndf})$ is the cumulative distribution function of a chi-squared distribution which we can calculate using `scipy.stats.chi2.cdf`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_pvalue(chi2, ndf):\n",
    "    return 1 - scipy.stats.chi2.cdf(chi2, ndf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_pvalue(\n",
    "    f_chi2(f_linear, pfit, x_data, y_data, yerr_data),\n",
    "    len(x_data) - len(pfit)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is rather low, again indicating a bad fit!\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Exercise</b> Fit a quadratic function to the data. What is $\\chi^2 / \\mathrm{ndf}$ now? What is the p-value?\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
